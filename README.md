## Confluence AI Chrome Extension (Local-first)

**Что это такое**

- **Local-first расширение** для Chrome поверх Confluence (Cloud/Server).
- **Индексирует страницы локально** в IndexedDB.
- Позволяет **искать по нескольким space** (по ключу пространства).
- Делает **выжимки и синтез** через ваш **локальный OpenAI-совместимый endpoint** (Ollama, LM Studio, локальный сервер).
- **Не отправляет контент страниц во внешние коммерческие облака** — все запросы идут только на указанный вами endpoint.

---

### Архитектура (кратко)

- `content script` (`src/content/confluence.ts`):
  - Читает DOM Confluence-страницы.
  - Извлекает текст, метаданные (URL, space key, page id).
  - Отправляет payload в background.
- `background service worker` (`src/background/index.ts`):
  - Принимает события индексации.
  - Сохраняет страницы в **IndexedDB** (`src/storage/indexdb.ts`).
  - Обрабатывает запросы поиска и суммаризации от UI.
- `UI (side panel)` (`src/ui/panel.html`, `panel.ts`):
  - Поле поиска по локальному индексу (keyword search).
  - Список найденных страниц, выбор подмножеств.
  - Кнопка Summarize, которая вызывает локальную LLM.
- `LLM-клиент` (`src/llm/client.ts`, `prompts.ts`):
  - Работает по **OpenAI Chat Completions API**.
  - BYOM: вы сами задаете endpoint, модель и ключ.

Архитектурные принципы:

- **Local-first**: контент Confluence не покидает вашу машину.
- **Retrieval-first**: сначала поиск, потом (опционально) LLM.
- **BYOM**: Bring Your Own Model (Ollama/LM Studio/локальный OpenAI-совместимый endpoint).
- **Zero backend**: никакого отдельного сервера по умолчанию.

---

### Установка и локальный запуск

1. **Установить зависимости**

```bash
npm install
```

2. **Собрать расширение для разработки**

```bash
npm run dev
```

3. **Подключить в Chrome**

- Открыть `chrome://extensions`.
- Включить **Developer mode**.
- Нажать **Load unpacked**.
- Выбрать папку `dist/` в корне репозитория.

---

### Подключение локальной модели (BYOM)

Расширение ожидает **OpenAI-совместимый endpoint** (Chat Completions).

Примеры:

- **Ollama** c openai-proxy.
- **LM Studio** с API-режимом.
- Собственный self-hosted endpoint.

Настройка:

1. Откройте **side panel** расширения (иконка расширения → "Confluence AI Search" → "Open side panel").
2. В блоке **LLM config (local endpoint)** укажите:
   - **Endpoint**, например: `http://localhost:11434/v1/chat/completions`
   - **Model**, например: `llama3.1`
   - **API key** (опционально, если ваш сервер его требует).
3. Нажмите **Save**.

Те же настройки доступны на странице **Options** расширения.

---

### Проверка NDA-безопасности

Как убедиться, что данные не уходят "наружу":

- Весь контент Confluence:
  - читается `content script` в браузере,
  - сохраняется в IndexedDB локально,
  - используется только в рамках текущего профиля Chrome.
- LLM-клиент отправляет **только текст запроса и извлеченные фрагменты страниц** на **указанный вами endpoint**.
- Вы сами контролируете:
  - куда указывает `llmEndpoint` (например, `localhost`),
  - какая модель используется,
  - нужен ли API-ключ.

Рекомендации:

- Использовать **только локальные** или **внутренние** (on-premise) endpoint'ы.
- Не ставить в `llmEndpoint` публичные SaaS-сервисы, если это нарушает NDA.
- При необходимости использовать корпоративный прокси, управляемый вашей компанией.

---

### Минимальный сценарий использования (MVP)

1. Открыть любую страницу Confluence.
2. Убедиться, что расширение установлено и активировано.
3. Подождать пару секунд — страница будет **проиндексирована** автоматически.
4. Открыть **side panel**.
5. Ввести поисковый запрос:
   - выполняется keyword search по локально сохраненным страницам.
6. Выбрать одну или несколько страниц в результатах.
7. Нажать **Summarize selected**:
   - расширение вызывает ваш локальный LLM,
   - генерирует выжимку и список ссылок на источники.

---

### Тестирование

Минимум, который стоит сделать:

- **Ручные тесты**:
  - Открыть несколько реальных/тестовых страниц Confluence.
  - Проверить, что они появляются в результатах поиска.
  - Проверить корректность ссылок и space key.
  - Проверить работу суммаризации при подключенном локальном LLM.
- **Fake Confluence page**:
  - Создать локальный HTML-файл со структурой, похожей на Confluence (`#main-content`, `.ak-renderer-document`).
  - Открыть его через `file://` или локальный dev-сервер и проверить, что контент корректно извлекается.

Дополнительно можно добавить автоматические тесты в директорию `tests/`.

---

### CI/CD

**GitHub Actions**

- В `.github/workflows/ci.yml` настроен пайплайн, который:
  - устанавливает зависимости,
  - собирает проект (`npm run build`).

**GitLab CI**

- В `.gitlab-ci.yml` настроен пайплайн с этапами:
  - `lint` (плейсхолдер),
  - `build` (сборка расширения).

Оба пайплайна работают без реальных внешних LLM и не нарушают принципов Local-first/BYOM.

